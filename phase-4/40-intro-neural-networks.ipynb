{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 40: Introduction to Neural Networks\n",
    "\n",
    "1. What is a **perceptron**?\n",
    "2. Multilayer perceptrons (neural networks!)\n",
    "3. Backpropagation\n",
    "4. Building our first neural network on Google Colab\n",
    "\n",
    "Check out this \"cheat sheet\": [These are all neural networks!](https://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-3fb6f2367464)\n",
    "\n",
    "<img src='resources/move_on.jpg' width=500/>\n",
    "\n",
    "## Applications of Neural Networks\n",
    "\n",
    "- Clustering\n",
    "- Pattern Recognition\n",
    "- Image Recognition (CNN)\n",
    "- Time Series Forecasting (RNN)\n",
    "- Audio/Video/Image Generation (GAN) \n",
    "\n",
    "#### Limitations\n",
    "- Good for prediction bad for inference \n",
    "- Computationally expensive "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression as a Perceptron\n",
    "\n",
    "* This is **one row of data**, each input is a different feature\n",
    "* Weights are determined through gradient descent\n",
    "* The **bias** term is our logistic regression intercept term\n",
    "* The **activation function** is the sigmoid function that forces output values between 0 and 1\n",
    "* Output is our classification result\n",
    "\n",
    "![](https://miro.medium.com/max/1280/1*8VSBCaqL2XeSCZQe_BAyVA.jpeg)\n",
    "\n",
    "\n",
    "* The perceptron algorithm is about learning the weights for inputs in order to draw a **linear decision boundary** that allows us to discriminate between two linearly separable classes\n",
    "* A perceptron takes in inputs, sums them up with weights, adds a bias, applies some activation function --> output\n",
    "* You can have different activation functions (sigmoid, tanh, ReLu, etc.)\n",
    "* Many perceptrons put together create a neural network\n",
    "\n",
    "<img src='resources/perceptron_binary.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptrons\n",
    "\n",
    "<img src='resources/non-linear-meme.webp'/>\n",
    "<img src='resources/non-linear.png'/>\n",
    "\n",
    "![fnn](resources/First_network.jpg)\n",
    "\n",
    "* Each node in the hidden layer works like a single perceptron - each node assigns different weights and biases to every row's inputs \n",
    "* Each node transforms the inputs and passes that through an activation function\n",
    "* The outputs from the activation functions are aggregated to an output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "    - Sigmoid\n",
    "    - Softmax\n",
    "    - Tanh\n",
    "    - ReLu\n",
    "    - elu\n",
    "    \n",
    "*In most cases you can use the ReLu activation function (or one of its variants) in the hidden layers. For the output layer, the softmax activation function is generally good for multiclass problems and the sigmoid function for binary classification problems.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "* Backpropagation is the process of picking the optimal weights of any **forward-feeding** neural network\n",
    "* Backpropagation is a procedure that uses gradient descent to propagate error terms backwards\n",
    "\n",
    "<img src='resources/back.png' width=600/>\n",
    "\n",
    "### Gradient Descent in Neural Networks\n",
    "Neural Nets are usually implemented at scale with large sets of data, therefore optimizing for speed becomes a big concern. Gradient descent can take a very **very** long time to run if we use a single training example every time to update the weights and biases. Therefore, we usually use batch-mode:\n",
    "\n",
    "- **Batch**: \n",
    "In batch gradient descent, we pass all of the training examples through the forward propagation stage before using backpropagation to compute the weights and biases\n",
    "\n",
    "- **Epoch**: \n",
    "An epoch is when you're done passing all training examples through the forward propagation\n",
    "\n",
    "\n",
    "##### Types of Gradient Descent\n",
    "- Stochastic Gradient Descent \n",
    "\n",
    "SGD calculates the error and update the weight after training each observation in the training set. \n",
    "\n",
    "- Batch Gradient Descent\n",
    "\n",
    "Batch calculates the error after each example is trained, but only updates the weight after all of the observations have been trained\n",
    "\n",
    "- Mini-Batch Gradient Descent\n",
    "\n",
    "Mini-batch is a compromise between batch and SGD - it splits the training examples into mini batches, and calculates the error and update the weight after each iteration of the mini batches are done training. \n",
    "\n",
    "\n",
    "### Random Initialization \n",
    "When we feed the node values forward through layers, we intialize the weights with random values and biases to be zero. We do not initialize the weights to be 0 because it would cause the training to be pointless because all weights would end up being the same. You also do not want a very large initial weights because that would saturate the value of activation function, causing taking the gradient of the activation function to be hard (only when you use sigmoid and tanh though)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras on Google Colab\n",
    "https://colab.research.google.com/drive/175P-oUi2VRZQx0isy5Yum-WxueYWOQlB?usp=sharing\n",
    "\n",
    "Keras is the default choice for beginner deep learners due to its user-friendly structure and easy implementation. It is built upon Tensorflow. The four steps to building your neural net models are:\n",
    "1. Specifying the architecture \n",
    "    - how many layers?\n",
    "    - how many nodes in hidden layers?\n",
    "    - what activation functions should you use?\n",
    "2. Compile your model \n",
    "    - specify the cost function and details about how optimization works\n",
    "        - learning rate\n",
    "        - optimizer\n",
    "        - list of metrics to use\n",
    "3. Fit your model \n",
    "    - backpropagation and adjusting for model weights\n",
    "4. Make predictions\n",
    "\n",
    "See Sequential model guide [here](https://keras.io/getting-started/sequential-model-guide/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "* 3 Blue 1 Brown: Backpropagation: https://www.youtube.com/watch?v=Ilg3gGewQ5U&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=3\n",
    "* StatQuest Neural Networks: https://www.youtube.com/watch?v=CqOfi41LfDw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "247.443px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
