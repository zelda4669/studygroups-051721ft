{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 28: Bayesian Classification\n",
    "\n",
    "### Bringing Back Bayes\n",
    "\n",
    "> \"Naive Bayes classifiers are linear classifiers that are known for being **simple yet very efficient**. The probabilistic model of naive Bayes classifiers is based on Bayes’ theorem, and the adjective naive comes from the assumption that the features in a dataset are **mutually independent**. In practice, the independence assumption is often violated, but naive Bayes classifiers **still tend to perform very well** under this unrealistic assumption. Especially for small sample sizes, naive Bayes classifiers can outperform the more powerful alternatives.\"\n",
    "\n",
    "[Source: Sebasitian Raschka: Naive Bayes and Text Classification](https://sebastianraschka.com/Articles/2014_naive_bayes_1.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sports = ['the match was close',\n",
    "          'the coaches agreed on strategy',\n",
    "          'played in a sold out stadium']\n",
    "\n",
    "politics = ['world leaders met last week',\n",
    "            'the election was close',\n",
    "            'the officials agreed on a compromise']\n",
    "\n",
    "test_statement = 'world leaders agreed to fund the stadium'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revisiting the theorem itself:\n",
    "\n",
    "<img src=\"resources/bayes_theorem.svg\" width=\"300\">\n",
    "\n",
    "<img src=\"resources/naive_bayes_icon.png\" width = \"400\">\n",
    "\n",
    "___\n",
    "\n",
    "<img src=\"resources/another_one.png\" width = \"600\">\n",
    "\n",
    "### In the context of our problem:\n",
    "\n",
    "$$ P(politics | document) = \\frac{P(document|politics)P(politics)}{P(document)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking the Equation Down\n",
    "\n",
    "### How should we calculate $ P(politics) $ ?\n",
    "\n",
    "This is essentially the distribution of the probability of either type of article. We have three of each type of article in our current set of example documents, therefore, we assume that there is an equal probability of either type.\n",
    "\n",
    "\n",
    "$$ P(politics) = \\frac{\\# politics\\ documents}{\\# all\\ documents} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(politics, '\\n')\n",
    "\n",
    "p_politics = len(politics)/(len(politics) + len(sports))\n",
    "print('P(politics) = ', p_politics)\n",
    "\n",
    "p_sports = len(sports)/(len(politics) + len(sports))\n",
    "print('P(sports) = ', p_sports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How should we calculate $ P(document | politics) $ ?\n",
    "\n",
    "We need to break the phrases down into individual words - with the hope that these words actually tell us more, since we likely have never seen this exact document before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(phrase | politics) = \\prod_{i=1}^{d} P(word_{i} | politics) $$\n",
    "$$ P(word_{i} | politics) = \\frac{\\#\\ of\\ word_{i}\\ in\\ politics\\ docs} {\\#\\ of\\ total\\ words\\ in\\ politics\\ docs} $$\n",
    "\n",
    "What if you see a word in your test statement that your model hasn't encountered before? The numerator becomes zero! \n",
    "\n",
    "#### Enter: Laplace Smoothing\n",
    "\n",
    "$$ P(word_{i} | politics) = \\frac{\\#\\ of\\ word_{i}\\ in\\ politics\\ docs + \\alpha} {\\#\\ of\\ total\\ words\\ in\\ politics\\ docs + \\alpha d} $$\n",
    "\n",
    "This correction process is called Laplace Smoothing:\n",
    "\n",
    "- d : number of features (in this instance total number of vocabulary words)\n",
    "- $\\alpha$ can be any number greater than 0 (it is usually 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How should we calculate $ P(document) $ ?\n",
    "\n",
    "We don't have to, because the P(document) doesn't change whether we're looking at sports or politics, we're just going to compare the numerator of these to see which is bigger between sports or politics!\n",
    "\n",
    "### So why is this 'naive' ?\n",
    "\n",
    "> \"Naive Bayes (NB) is ‘naive’ because it makes the assumption that features of a measurement are independent of each other. This is naive because it is (almost) never true.\"\n",
    "\n",
    "[Source - 'What's So Naive About Naive Bayes?', a Towards Data Science blog post all about this](https://towardsdatascience.com/whats-so-naive-about-naive-bayes-58166a6a9eba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying our equations:\n",
    "\n",
    "| word       | frequency in politics | frequency in sports |\n",
    "| ---------- | --------------------- | ------------------- |\n",
    "| the        |  2                    | 2                   |\n",
    "| match      |  0                    | 1                   |\n",
    "| was        |  1                    | 1                   |\n",
    "| close      |  1                    | 1                   |\n",
    "| coaches    |  0                    | 1                   |\n",
    "| agreed     |  1                    | 1                   |\n",
    "| on         |  1                    | 1                   |\n",
    "| strategy   |  0                    | 1                   |\n",
    "| played     |  0                    | 1                   |\n",
    "| in         |  0                    | 1                   |\n",
    "| a          |  1                    | 1                   |\n",
    "| sold       |  0                    | 1                   |\n",
    "| out        |  0                    | 1                   |\n",
    "| stadium    |  0                    | 1                   |\n",
    "| world      |  1                    | 0                   |\n",
    "| leaders    |  1                    | 0                   |\n",
    "| met        |  1                    | 0                   |\n",
    "| last       |  1                    | 0                   |\n",
    "| week       |  1                    | 0                   |\n",
    "| election   |  1                    | 0                   |\n",
    "| officials  |  1                    | 0                   |\n",
    "| compromise |  1                    | 0                   |\n",
    "\n",
    "> Test sentence: 'world leaders agreed to fund the stadium'\n",
    "\n",
    "| word    | $ P( word | politics) $                | $ P( word | sports) $                  |\n",
    "| ------- | -------------------------------------- | -------------------------------------- |\n",
    "| world   | $\\frac{1 + 1}{15 + 30} = \\frac{2}{45}$ | $\\frac{0 + 1}{15 + 30} = \\frac{1}{45}$ |\n",
    "| leaders | $\\frac{1 + 1}{15 + 30} = \\frac{2}{45}$ | $\\frac{0 + 1}{15 + 30} = \\frac{1}{45}$ |\n",
    "| agreed  | $\\frac{1 + 1}{15 + 30} = \\frac{2}{45}$ | $\\frac{1 + 1}{15 + 30} = \\frac{2}{45}$ |\n",
    "| to      | $\\frac{0 + 1}{15 + 30} = \\frac{1}{45}$ | $\\frac{0 + 1}{15 + 30} = \\frac{1}{45}$ |\n",
    "| fund    | $\\frac{0 + 1}{15 + 30} = \\frac{1}{45}$ | $\\frac{0 + 1}{15 + 30} = \\frac{1}{45}$ |\n",
    "| the     | $\\frac{2 + 1}{15 + 30} = \\frac{3}{45}$ | $\\frac{2 + 1}{15 + 30} = \\frac{3}{45}$ |\n",
    "| stadium | $\\frac{0 + 1}{15 + 30} = \\frac{1}{45}$ | $\\frac{1 + 1}{15 + 30} = \\frac{2}{45}$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, we want\n",
    "\n",
    "$ P(document|sports) * P(sports) $ **vs.** $ P(document|politics) * P(politics) $\n",
    "\n",
    "We already calculated $P(sports)$ and $P(politics)$ above - they're both 50% since we have an equal number of documents in each. But the $P(document | label)$ for the two labels is going to require breaking out each word to find the likelihood that each word is in each category (plus Laplacian smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_maker(category):\n",
    "    \"\"\"\n",
    "    returns the vocabulary for a given type of article\n",
    "    \"\"\"\n",
    "    vocab_category = set()\n",
    "    for art in category:\n",
    "        words = art.split()\n",
    "        for word in words:\n",
    "            vocab_category.add(word)\n",
    "    return vocab_category\n",
    "        \n",
    "voc_sports = vocab_maker(sports)\n",
    "voc_pol = vocab_maker(politics)\n",
    "total_vocabulary = voc_sports.union(voc_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocab_count = len(total_vocabulary) # useful for laplacian smoothing\n",
    "total_sports_count = len(voc_sports)\n",
    "total_politics_count = len(voc_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_number_words_in_category(phrase, category):\n",
    "    '''\n",
    "    returns number of words in the phrase previously found in the category\n",
    "    \n",
    "    inputs:\n",
    "    phrase - string, test phrase to classify\n",
    "    category - list, all training phrases associated with that category\n",
    "    \n",
    "    output:\n",
    "    word_count - default dictionary, with each word in the phrase as a key \n",
    "                 with a value of the number of times the words have \n",
    "                 appeared in the category in the train set\n",
    "    '''\n",
    "    # gets each word out - statement is a list object now\n",
    "    statement = phrase.split()\n",
    "    \n",
    "    # creating one big string from the provided category list\n",
    "    str_category=' '.join(category)\n",
    "    # splitting now so it's a single list of the words found in the category\n",
    "    cat_word_list = str_category.split()\n",
    "    # default dict allows us to create new keys easily\n",
    "    word_count = defaultdict(int) \n",
    "    \n",
    "    for word in statement:\n",
    "        for cat_word in cat_word_list:\n",
    "            if word == cat_word:\n",
    "                word_count[word] +=1\n",
    "            else:\n",
    "                word_count[word] # here's the part that works because default dict\n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sports_word_count = find_number_words_in_category(test_statement,sports)\n",
    "test_sports_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_politic_word_count = find_number_words_in_category(test_statement,politics)\n",
    "test_politic_word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(politics | article) = P(politics) x \\prod_{i=1}^{d} P(word_{i} | politics) $$\n",
    "\n",
    "$$ P(word_{i} | politics) = \\frac{\\#\\ of\\ word_{i}\\ in\\ politics\\ docs + \\alpha} {\\#\\ of\\ total\\ words\\ in\\ politics\\ docs + \\alpha d} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_likelihood(category_count, test_category_count, alpha):\n",
    "    \n",
    "    num = np.product(np.array(list(test_category_count.values())) + alpha)\n",
    "    denom = (category_count + total_vocab_count*alpha)**(len(test_category_count))\n",
    "    \n",
    "    return num/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_sports = find_likelihood(total_sports_count,test_sports_word_count,1)\n",
    "likelihood_politics = find_likelihood(total_politics_count,test_politic_word_count,1)\n",
    "\n",
    "print(likelihood_sports)\n",
    "print(likelihood_politics)\n",
    "\n",
    "(likelihood_politics * p_politics) > (likelihood_sports * p_sports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Summary...\n",
    "\n",
    "### Pros:\n",
    "\n",
    "* It is an efficient way to predict classes of your test data set. It performs well in multiclass prediction\n",
    "* When assumption of independence holds, a Naive Bayes classifier requires less training data and can perform better than models like logistic regression.\n",
    "* Performs better with categorical inputs. For numerical input, one has to assume a normal distribution.\n",
    "\n",
    "### Cons:\n",
    "\n",
    "* Naive Bayes is also known as a bad estimator, so the probability outputs from predict_proba are not to be taken too seriously\n",
    "* We are assuming of independent predictors, but in real life, it is almost impossible that we get a set of predictors which are completely independent (amazingly, still works a lot of the time though!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score, f1_score, plot_confusion_matrix, confusion_matrix\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching our data\n",
    "news_train = fetch_20newsgroups(subset='train', \n",
    "                                categories = ['rec.sport.baseball', \n",
    "                                              'talk.politics.misc'])\n",
    "news_test = fetch_20newsgroups(subset='test', \n",
    "                               categories = ['rec.sport.baseball', \n",
    "                                              'talk.politics.misc'])\n",
    "\n",
    "# collecting data in dataframe\n",
    "df_train = pd.DataFrame()\n",
    "df_train['Data'] = news_train.data\n",
    "df_train['Target'] = news_train.target\n",
    "\n",
    "df_test = pd.DataFrame()\n",
    "df_test['Data'] = news_test.data\n",
    "df_test['Target'] = news_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = dict(enumerate(news_test.target_names))\n",
    "target_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.Target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.Target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Data'][42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing text\n",
    "\n",
    "We'll talk more about this when we get to NLP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a Count Vectorizer\n",
    "# Goes through each doc and counts how many of each word\n",
    "vectorizer = CountVectorizer()\n",
    "# Fitting and transforming our train data\n",
    "X_train = vectorizer.fit_transform(df_train['Data']).toarray() # to array is just for the model later\n",
    "# Just transforming our test data\n",
    "X_test = vectorizer.transform(df_test['Data']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does this look like?\n",
    "X_train_vectorized = pd.DataFrame(X_train, columns=vectorizer.get_feature_names())\n",
    "X_train_vectorized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorized.iloc[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train['Target']\n",
    "y_test = df_test['Target']\n",
    "\n",
    "# Instantiating our model - just using default values\n",
    "model = GaussianNB()\n",
    "# Fitting our model\n",
    "model.fit(X_train,y_train)\n",
    "# Making predictions on our test set\n",
    "y_preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model, X_train, y_train, display_labels = target_classes.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tpreds = model.predict(X_train)\n",
    "print(f'Naive Bayes Test Accuracy: {accuracy_score(y_train, y_tpreds):.4f}')\n",
    "print(f'Naive Bayes Test Precision: {precision_score(y_train, y_tpreds):.4f}')\n",
    "print(f'Naive Bayes Test Recall: {recall_score(y_train, y_tpreds):.4f}')\n",
    "print(f'Naive Bayes Test F1-Score: {f1_score(y_train, y_tpreds):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model, X_test, y_test, display_labels = target_classes.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Naive Bayes Test Accuracy: {accuracy_score(y_test, y_preds):.4f}')\n",
    "print(f'Naive Bayes Test Precision: {precision_score(y_test, y_preds):.4f}')\n",
    "print(f'Naive Bayes Test Recall: {recall_score(y_test, y_preds):.4f}')\n",
    "print(f'Naive Bayes Test F1-Score: {f1_score(y_test, y_preds):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
