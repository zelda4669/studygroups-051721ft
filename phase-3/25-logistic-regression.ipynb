{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 25: Logistic Regression\n",
    "\n",
    "## But first, an overview of Machine Learning\n",
    "\n",
    "\n",
    "\n",
    "![](https://wordstream-files-prod.s3.amazonaws.com/s3fs-public/styles/simple_image/public/images/machine-learning1.png?SnePeroHk5B9yZaLY7peFkULrfW8Gtaf&itok=yjEJbEKD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Logistic Regression? \n",
    "\n",
    "![](https://miro.medium.com/max/400/1*zLfpo6F_Bfi6uvRL6iLX_Q.jpeg)\n",
    "It belongs to a class of predictive models called _Generalized Linear Models_. All of these models have 2 things in common: They all define significant relationships between independent/dependent variables and they indicate the strength of the relationships. \n",
    "\n",
    "Different from Linear regression -- it can predict the probabilities associated with **a success or a failure**. Is this email likely spam? What is the probability that this citizen will vote Republican? Is this homeowner likely to default on their mortgage? Is this person likely to buy our product? Is this tumor likely to be cancerous or benign?\n",
    "\n",
    "### Assumptions \n",
    "**Logistic Regression Assumptions:**\n",
    "\n",
    "* Binary logistic regression requires the dependent variable to be binary.\n",
    "* Only the meaningful variables should be included.\n",
    "* The independent variables should be independent of each other. That is, the model should have little or no multi-collinearity.\n",
    "* The independent variables are linearly related to the log odds.\n",
    "* Logistic regression requires quite large sample sizes.\n",
    "\n",
    "### Key differences from Linear Regression:\n",
    "* GLM does not assume a linear relationship between dependent and independent variables. However, it assumes a linear relationship between link function and independent variables in logit model.\n",
    "\n",
    "* The dependent variable need not to be normally distributed.\n",
    "\n",
    "* It does not uses OLS (Ordinary Least Square) for parameter estimation. Instead, it uses maximum likelihood estimation (MLE).\n",
    "\n",
    "* Errors need to be independent but not normally distributed.\n",
    "\n",
    "### Logistic Regression Equation\n",
    "\n",
    "![](https://miro.medium.com/max/571/0*tGVPGu3aa1rhTdfl.png)\n",
    "Let's say we've constructed our best-fit line, i.e. our linear predictor, $\\hat{L} = \\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n$.\n",
    "\n",
    "#### The Sigmoid Function\n",
    "\n",
    "Consider the following transformation:\n",
    "$\\large\\hat{y} = \\Large\\frac{1}{1 + e^{-\\hat{L}}} \\large= \\Large\\frac{1}{1 + e^{-(\\beta_0 + ... + \\beta_nx_n)}}$. This is called the sigmoid function.\n",
    "\n",
    "This function squeezes our predictions between 0 and 1. \n",
    "\n",
    "Suppose I'm building a model to predict whether a plant is poisonous or not, based perhaps on certain biological features of its leaves. \n",
    "* I'll let '1' indicate a poisonous plant and '0' indicate a non-poisonous plant.\n",
    "* Now I'm forcing my predictions to be between 0 and 1, so suppose for test plant $P$ I get some value like 0.19.\n",
    "* I can naturally understand this as the probability that $P$ is poisonous.\n",
    "* If I truly want a binary prediction, I can simply round my score appropriately.\n",
    "\n",
    "How do we fit a line to our dependent variable if its values are already stored as probabilities? We can use the inverse of the sigmoid function, and just set our regression equation equal to that. The inverse of the sigmoid function is called the logit function, and it looks like this:\n",
    "\n",
    "$$\\large f(y) = \\ln\\left(\\frac{y}{1 - y}\\right)$$\n",
    "\n",
    "Notice that the domain of this function is $(0, 1)$.\n",
    "\n",
    "Quick proof that logit and sigmoid are inverse functions:\n",
    "\n",
    "$\\hspace{170mm}x = \\frac{1}{1 + e^{-y}}$;\n",
    "$\\hspace{170mm}$so $1 + e^{-y} = \\frac{1}{x}$;\n",
    "$\\hspace{170mm}$so $e^{-y} = \\frac{1 - x}{x}$;\n",
    "$\\hspace{170mm}$so $-y = \\ln\\left(\\frac{1 - x}{x}\\right)$;\n",
    "$\\hspace{170mm}$so $y = \\ln\\left(\\frac{x}{1 - x}\\right)$.)\n",
    "\n",
    "Our regression equation will now look like this:\n",
    "\n",
    "$\\large\\ln\\left(\\frac{y}{1 - y}\\right) = \\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import some data to play with\n",
    "from sklearn import datasets\n",
    "\n",
    "# For our modeling steps\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    data= np.c_[iris['data'], iris['target']],\n",
    "    columns= iris['feature_names'] + ['target']\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a large figure\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Iterating over the different\n",
    "for i in range(0, 4):\n",
    "    # Figure number starts at 1\n",
    "    ax = fig.add_subplot(2, 2, i+1)\n",
    "    # Add a title to make it clear what each subplot shows\n",
    "    plt.title(df.columns[i])\n",
    "    # Use alpha to better see crossing pints\n",
    "    ax.scatter(df['target'], df.iloc[:,i], c='teal', alpha=0.1)\n",
    "    # Only show the tick marks for each target\n",
    "    plt.xticks(df.target.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,:-1]\n",
    "y = df.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=27)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(fit_intercept = False, C = 1e12, solver='lbfgs', multi_class='auto')\n",
    "logreg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = logreg.predict(X_test)\n",
    "y_hat_train = logreg.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_train == y_hat_train\n",
    "\n",
    "print('Number of values correctly predicted:')\n",
    "print(pd.Series(residuals).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test == y_hat_test\n",
    "\n",
    "print('Number of values correctly predicted: ')\n",
    "print(pd.Series(residuals).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Classification Models \n",
    "\n",
    "For classification problems, the target is a categorical variable. This means that we can simply count the number of times that our model predicts the correct category and the number of times that it predicts something else.\n",
    "\n",
    "We can visualize this by means of a **confusion matrix**, a tabular representation of Actual vs Predicted values.\n",
    "![](https://miro.medium.com/max/350/0*rhntpf-9O0A5HjCP)\n",
    "\n",
    "**The metrics for evaluating your models performance can be drawn from this matrix** \n",
    "\n",
    "* Accuracy = $\\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "\n",
    "* Recall = $\\frac{TP}{TP + FN}$\n",
    "\n",
    "* Precision = $\\frac{TP}{TP + FP}$\n",
    "\n",
    "* F-1 Score = $\\frac{2PrRc}{Pr + Rc}$ = $\\frac{2TP}{2TP + FP + FN}$ \n",
    "\n",
    "**General Lessons**: \n",
    "First, let's make some general observations about the metrics we've so far defined.\n",
    "\n",
    "**Accuracy:**\n",
    "\n",
    "   * **Pro:** Takes into account both false positives and false negatives.\n",
    "\n",
    "   * **Con:** Can be misleadingly high when there is a significant class imbalance. (A lottery-ticket predictor that always predicts a loser will be highly accurate.)\n",
    "\n",
    "**Recall:**\n",
    "\n",
    "   * **Pro:** Highly sensitive to false negatives.\n",
    "\n",
    "   * **Con:** No sensitivity to false positives.\n",
    "\n",
    "**Precision:**\n",
    "\n",
    "   * **Pro:** Highly sensitive to false positives.\n",
    "\n",
    "   * **Con:** No sensitivity to false negatives.\n",
    "\n",
    "\n",
    "**F-1 Score:**\n",
    "\n",
    "Harmonic mean of recall and precision.\n",
    "\n",
    "**AIC (Akaike Information Criteria**) — The analogous metric of adjusted R² in logistic regression is AIC. AIC is the measure of fit which penalizes model for the number of model coefficients. Therefore, we always prefer model with minimum AIC value.\n",
    "\n",
    "**ROC Curve:** Receiver Operating Characteristic (ROC) summarizes the model’s performance by evaluating the trade-offs between true positive rate (sensitivity) and false positive rate (1- specificity). For plotting ROC, it is advisable to assume p > 0.5 since we are more concerned about success rate. ROC summarizes the predictive power for all possible values of p > 0.5. The area under curve (AUC), referred to as index of accuracy (A) or concordance index, is a perfect performance metric for ROC curve. Higher the area under curve, better the prediction power of the model. Below is a sample ROC curve. The ROC of a perfect predictive model has TP equals 1 and FP equals 0. This curve will touch the top left corner of the graph.\n",
    "![](https://miro.medium.com/max/300/0*20UWoOC5Gi4SdbAw.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(logreg, X_train, y_train,\n",
    "                     cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(logreg, X_test, y_test,\n",
    "                     cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_train_pred = logreg.predict(X_train)\n",
    "y_test_pred = logreg.predict(X_test)\n",
    "\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "y_bin = label_binarize(y, classes=[0, 1, 2])\n",
    "\n",
    "y_bin_train, y_bin_test = train_test_split(y_bin, test_size=0.2, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scikit-learn's built in roc_curve method returns the fpr, tpr, and thresholds\n",
    "# for various decision boundaries given the case member probabilites\n",
    "\n",
    "# First calculate the probability scores of each of the datapoints:\n",
    "y_score = logreg.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "print(y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting and estimation of FPR, TPR\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(3):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_bin_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "colors = cycle(['blue', 'red', 'green'])\n",
    "for i, color in zip(range(3), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=1.5, label='ROC curve of class {0} (area = {1:0.2f})' ''.format(i+1, roc_auc[i]))\n",
    "plt.plot([0, 1], [0, 1], 'k-', lw=1.5)\n",
    "plt.xlim([-0.05, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic for multi-class data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources:\n",
    "\n",
    "* StatQuest: Logistic Regression (HIGHLY recommend for an overview of the math behind logistic regression)\n",
    "    * https://www.youtube.com/watch?v=yIYKR4sgzI8\n",
    "    * https://www.youtube.com/watch?v=vN5cNN2-HWE\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it out!\n",
    "\n",
    "* https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset\n",
    "\n",
    "Keep a running notebook of different algorithms predicting on this dataset!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "244.55px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
